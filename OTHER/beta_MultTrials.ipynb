{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c9392a2-17e5-4859-89c5-f96e18b34c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lova\\AppData\\Local\\Temp\\ipykernel_20144\\1560085282.py:2: DeprecationWarning: The import path 'nilearn.input_data' is deprecated in version 0.9. Importing from 'nilearn.input_data' will be possible at least until release 0.13.0. Please import from 'nilearn.maskers' instead.\n",
      "  from nilearn.input_data import NiftiSpheresMasker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lova\\.conda\\envs\\neuro_ml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# requirements: nilearn, nibabel, numpy, pandas\n",
    "from nilearn.input_data import NiftiSpheresMasker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from nilearn import datasets, masking, image\n",
    "from nilearn.masking import compute_epi_mask, apply_mask, unmask\n",
    "from nilearn.plotting import plot_stat_map, plot_glass_brain, show\n",
    "from nilearn.image import resample_img, mean_img\n",
    "import nibabel as nib\n",
    "import optuna\n",
    "import pywt\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from torch.utils.data import Dataset\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "# -----------------------------\n",
    "# Device check\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b756fef-21e7-4c9d-a5de-9a0fb68618f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of betas: (79, 95, 79)\n",
      "Shape of mask1: (79, 95, 79), Shape of mask2: (79, 95, 79)\n"
     ]
    }
   ],
   "source": [
    "pattern_dir = \"Data/beta_images_activation_async\"\n",
    "betas_path = os.path.join(pattern_dir, 'single_trial_sess1/beta_0001.nii')\n",
    "R_4D = image.load_img(betas_path)\n",
    "print(f\"Shape of betas: {R_4D.shape}\")\n",
    "\n",
    "mask_img1 = image.load_img(os.path.join(pattern_dir, 'single_trial_sess1/mask.nii'))\n",
    "mask_img2 = image.load_img(os.path.join(pattern_dir, 'single_trial_sess2/mask.nii'))\n",
    "print(f\"Shape of mask1: {mask_img1.shape}, Shape of mask2: {mask_img2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a87de2-7be4-49d6-9941-5df314a8299c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_beta_dataset(root):\n",
    "    root = Path(root)\n",
    "    base = root / \"beta_images_activation_async\"\n",
    "\n",
    "    sess_dirs = [\n",
    "        base / \"single_trial_sess1\",\n",
    "        base / \"single_trial_sess2\",\n",
    "    ]\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    # all label CSVs\n",
    "    csv_files = sorted(base.glob(\"beta_labels_subject*_session*.csv\"))\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            fname = row[\"file\"]\n",
    "            label = row[\"class\"]\n",
    "\n",
    "            # look in both session folders\n",
    "            found_path = None\n",
    "            for d in sess_dirs:\n",
    "                candidate = d / fname\n",
    "                if candidate.exists():\n",
    "                    found_path = candidate\n",
    "                    break\n",
    "\n",
    "            if found_path is None:\n",
    "                raise FileNotFoundError(f\"Could not find {fname} in session folders\")\n",
    "\n",
    "            # load nii â†’ keep 3D shape\n",
    "            img = nib.load(str(found_path)).get_fdata()\n",
    "\n",
    "            # FIX NaNs HERE\n",
    "            img = np.nan_to_num(img, nan=0.0)\n",
    "\n",
    "            X.append(img)\n",
    "            y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "X_all, y_all = load_beta_dataset(\"Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b773694-b23f-4fb3-9cee-bd43ca510a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (320, 79, 95, 79), y shape: (320,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape: {X_all.shape}, y shape: {y_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022686c5-4ca0-494c-8500-6c1ee9de4812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (272, 79, 95, 79)\n",
      "Test shape: (48, 79, 95, 79)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hold-out test BEFORE any Optuna tuning\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=0.15,\n",
    "    stratify=y_all,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train_val.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "253f7d3c-8603-435f-b99d-cbc61e52bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BimodalDataset(Dataset):\n",
    "    def __init__(self, fmri, eeg, labels, label_encoder=None):\n",
    "        self.fmri = fmri\n",
    "        self.eeg = eeg\n",
    "\n",
    "        if label_encoder is None:\n",
    "            self.encoder = LabelEncoder()\n",
    "            self.labels = self.encoder.fit_transform(labels)\n",
    "        else:\n",
    "            self.encoder = label_encoder\n",
    "            self.labels = self.encoder.transform(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            \"fmri\": torch.tensor(self.fmri[i], dtype=torch.float32),\n",
    "            \"eeg\": torch.tensor(self.eeg[i], dtype=torch.float32),\n",
    "            \"label\": torch.tensor(self.labels[i], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def inverse_labels(self, encoded):\n",
    "        \"\"\"Return original labels from encoded integers\"\"\"\n",
    "        return self.encoder.inverse_transform(encoded)\n",
    "\n",
    "\n",
    "train_async = BimodalDataset(X_train_val, None, y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e24f7542-cc7e-4189-848f-202d7ced03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_async = BimodalDataset(X_test, None, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc7cb28e-e03e-4fb2-9385-6ffacb888632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fetch_atlas_aal] Dataset found in C:\\Users\\Lova\\nilearn_data\\aal_SPM12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Build AAL masker once\n",
    "_aal = datasets.fetch_atlas_aal()\n",
    "_aal_img = _aal.maps\n",
    "\n",
    "# Fit masker once on the atlas only\n",
    "_masker = NiftiLabelsMasker(labels_img=_aal_img, standardize=True)\n",
    "_masker.fit()\n",
    "\n",
    "def _fmri_to_aal_timeseries(fmri_4d):\n",
    "    # Ensure float32 to save RAM\n",
    "    fmri_4d = fmri_4d.astype(\"float32\", copy=False)\n",
    "\n",
    "    # Convert array â†’ NIfTI\n",
    "    fmri_nii = image.new_img_like(_aal_img, fmri_4d)\n",
    "\n",
    "    # Only transform (masker already fitted)\n",
    "    ts = _masker.transform(fmri_nii)   # (T, n_regions)\n",
    "    return ts.T                        # (n_regions, T)\n",
    "\n",
    "def convert_dataset_to_aal(train_ds, test_ds, verbose=True):\n",
    "    def convert_split(split, name):\n",
    "        out = []\n",
    "        for i, f in enumerate(split.fmri):\n",
    "            if verbose:\n",
    "                print(f\"{name}: {i+1}/{len(split.fmri)}\", end=\"\\r\", flush=True)\n",
    "            out.append(_fmri_to_aal_timeseries(f))\n",
    "        return np.stack(out, axis=0)\n",
    "\n",
    "    train_fmri = convert_split(train_ds, \"train\")\n",
    "    test_fmri  = convert_split(test_ds, \"test\")\n",
    "\n",
    "    new_train = BimodalDataset(\n",
    "        fmri=train_fmri,\n",
    "        eeg=train_ds.eeg,\n",
    "        labels=train_ds.inverse_labels(train_ds.labels)\n",
    "    )\n",
    "    new_test = BimodalDataset(\n",
    "        fmri=test_fmri,\n",
    "        eeg=test_ds.eeg,\n",
    "        labels=test_ds.inverse_labels(test_ds.labels)\n",
    "    )\n",
    "\n",
    "    return new_train, new_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5fa43a1-03f6-43f5-a3ab-b30e7b77811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 48/48272\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_async_aal, test_async_aal = convert_dataset_to_aal(train_async, test_async)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77c9c3c2-81c6-4927-8bb6-6d7b618db6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (272, 116)\n",
      "Test shape: (48, 116)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {train_async_aal.fmri.shape}\")\n",
    "\n",
    "print(f\"Test shape: {test_async_aal.fmri.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb52cf6c-e33f-4988-854a-7d14786504f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_with_cv(\n",
    "    X_train_val, y_train_val, X_test, y_test,\n",
    "    n_trials=30, n_splits=5, label_decoder=None\n",
    "):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    model_types = [\"svc\", \"linear_svc\", \"rf\"]\n",
    "    all_results = {}\n",
    "    N_samples, N_features = X_train_val.shape\n",
    "\n",
    "    # ===== Adaptive feature selection limits =====\n",
    "    # ANOVA percentile\n",
    "    if N_features <= 50:  # ROI regime\n",
    "        min_percent, max_percent = 1, 100\n",
    "    else:  # voxel regime\n",
    "        max_voxels = 30000\n",
    "        max_percent = int(100 * min(max_voxels, N_features) / N_features)\n",
    "        max_percent = min(max_percent, 100)\n",
    "        min_percent = 1\n",
    "   \n",
    "\n",
    "    # PCA/ICA component limits\n",
    "    max_pca = min(200, N_features, N_samples - 1)\n",
    "    max_ica = min(80, N_features, N_samples // 4)\n",
    "\n",
    "    for model_type in model_types:\n",
    "        print(f\"\\nðŸ”¹ OPTUNA + {n_splits}-Fold CV for: {model_type.upper()}\")\n",
    "\n",
    "        # ===== Objective function =====\n",
    "        def objective_wrapper(trial):\n",
    "            # Feature selection\n",
    "            feat_method = trial.suggest_categorical(\"feat_method\", [\"anova\", \"pca\", \"ica\"])\n",
    "\n",
    "            if feat_method == \"anova\":\n",
    "                percentile = trial.suggest_int(\"percentile\", min_percent, max_percent)\n",
    "                selector = SelectPercentile(f_classif, percentile=percentile)\n",
    "\n",
    "            elif feat_method == \"pca\":\n",
    "                n_components = trial.suggest_int(\"pca_n_components\", 10, 100)\n",
    "                selector = PCA(n_components=n_components, random_state=42)\n",
    "\n",
    "            else:  # ICA\n",
    "                n_components = trial.suggest_int(\"ica_n_components\", 10, 100)\n",
    "                selector = FastICA(n_components=n_components, random_state=42, max_iter=1000)\n",
    "\n",
    "            # Model selection\n",
    "            if model_type == \"svc\":\n",
    "                kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"sigmoid\"])\n",
    "                C = trial.suggest_float(\"C\", 1e-2, 1e1, log=True)\n",
    "\n",
    "                if kernel == \"linear\":\n",
    "                    model = SVC(kernel=\"linear\", C=C, random_state=42)\n",
    "                else:\n",
    "                    gamma = trial.suggest_float(\"gamma\", 1e-4, 1e-1, log=True)\n",
    "                    coef0 = trial.suggest_float(\"coef0\", 0.0, 1.0)\n",
    "                    degree = trial.suggest_int(\"degree\", 2, 6) if kernel == \"poly\" else 3\n",
    "                    model = SVC(kernel=kernel, C=C, gamma=gamma, coef0=coef0, degree=degree, random_state=42)\n",
    "\n",
    "            elif model_type == \"linear_svc\":\n",
    "                C = trial.suggest_float(\"C\", 1e-3, 1e2, log=True)\n",
    "                model = LinearSVC(C=C, max_iter=10000, random_state=42)\n",
    "\n",
    "            else:  # RandomForest\n",
    "                n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "                max_depth = trial.suggest_int(\"max_depth\", 2, 20)\n",
    "                min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 5)\n",
    "                min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 3)\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_split=min_samples_split,\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    random_state=42,\n",
    "                )\n",
    "\n",
    "            # Pipeline\n",
    "            pipeline = make_pipeline(StandardScaler(), selector, model)\n",
    "\n",
    "            # K-Fold CV\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "            scores = cross_val_score(\n",
    "                pipeline,\n",
    "                X_train_val,\n",
    "                y_train_val,\n",
    "                cv=cv,\n",
    "                scoring=\"f1_macro\",\n",
    "                n_jobs=1,\n",
    "            )\n",
    "\n",
    "            return scores.mean()\n",
    "\n",
    "        # ===== Run Optuna =====\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective_wrapper, n_trials=n_trials)\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best Params for {model_type} = {best_params}\")\n",
    "\n",
    "        # ===== Rebuild final pipeline =====\n",
    "        feat_method = best_params[\"feat_method\"]\n",
    "        if feat_method == \"anova\":\n",
    "            selector = SelectPercentile(f_classif, percentile=best_params[\"percentile\"])\n",
    "        elif feat_method == \"pca\":\n",
    "            selector = PCA(n_components=best_params[\"pca_n_components\"], random_state=42)\n",
    "        else:\n",
    "            selector = FastICA(n_components=best_params[\"ica_n_components\"], random_state=42, max_iter=1000)\n",
    "\n",
    "        # Model\n",
    "        if model_type == \"svc\":\n",
    "            if best_params[\"kernel\"] == \"linear\":\n",
    "                model = SVC(kernel=\"linear\", C=best_params[\"C\"], random_state=42)\n",
    "            else:\n",
    "                model = SVC(\n",
    "                    kernel=best_params[\"kernel\"],\n",
    "                    C=best_params[\"C\"],\n",
    "                    gamma=best_params.get(\"gamma\", \"scale\"),\n",
    "                    coef0=best_params.get(\"coef0\", 0),\n",
    "                    degree=best_params.get(\"degree\", 3),\n",
    "                    random_state=42,\n",
    "                )\n",
    "        elif model_type == \"linear_svc\":\n",
    "            model = LinearSVC(C=best_params[\"C\"], max_iter=10000, random_state=42)\n",
    "        else:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=best_params[\"n_estimators\"],\n",
    "                max_depth=best_params[\"max_depth\"],\n",
    "                min_samples_split=best_params[\"min_samples_split\"],\n",
    "                min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "        final_pipeline = make_pipeline(StandardScaler(), selector, model)\n",
    "        final_pipeline.fit(X_train_val, y_train_val)\n",
    "\n",
    "        # Test evaluation\n",
    "        y_pred = final_pipeline.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        print(f\"FINAL RESULTS {model_type.upper()}: acc={acc:.3f}, f1={f1:.3f}\")\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        ConfusionMatrixDisplay(cm, display_labels=label_decoder).plot(cmap=\"Blues\")\n",
    "        plt.show()\n",
    "\n",
    "        all_results[model_type] = {\n",
    "            \"pipeline\": final_pipeline,\n",
    "            \"acc\": acc,\n",
    "            \"f1\": f1,\n",
    "            \"cm\": cm,\n",
    "            \"best_params\": best_params,\n",
    "            \"study\": study,\n",
    "        }\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29fedb47-2401-4946-b60b-553c29e3c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optuna_with_bimodal_dataset(\n",
    "    train_dataset: BimodalDataset,\n",
    "    test_dataset: BimodalDataset,\n",
    "    n_trials=30,\n",
    "    n_splits=10,\n",
    "    use_temporal_flattener=False\n",
    "):\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "    from sklearn.decomposition import PCA, FastICA\n",
    "    from sklearn.svm import SVC, LinearSVC\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "    import optuna\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Convert datasets to arrays\n",
    "    def dataset_to_arrays(ds):\n",
    "        fmri = np.array([ds[i]['fmri'].numpy() for i in range(len(ds))])\n",
    "        X = fmri.reshape(len(ds), -1)  # flatten each 3D fMRI into 1D\n",
    "        y = np.array([ds[i]['label'].item() for i in range(len(ds))])\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    X_train, y_train = dataset_to_arrays(train_dataset)\n",
    "    X_test, y_test   = dataset_to_arrays(test_dataset)\n",
    "\n",
    "    N_samples, N_features = X_train.shape\n",
    "    min_percent = 1\n",
    "    max_voxels = 30000\n",
    "    max_percent = int(100 * min(max_voxels, N_features) / N_features)\n",
    "    max_percent = min(max_percent, 100)\n",
    "    max_pca = min(200, N_features, N_samples-1)\n",
    "    max_ica = min(80, N_features, N_samples//4)\n",
    "\n",
    "    model_types = [\"svc\", \"linear_svc\", \"rf\"]\n",
    "    all_results = {}\n",
    "\n",
    "    for model_type in model_types:\n",
    "        print(f\"\\nðŸ”¹ OPTUNA + {n_splits}-Fold CV for: {model_type.upper()}\")\n",
    "\n",
    "        def objective_wrapper(trial):\n",
    "            # Feature selection\n",
    "            feat_method = trial.suggest_categorical(\"feat_method\", [\"anova\", \"pca\", \"ica\"])\n",
    "            if feat_method == \"anova\":\n",
    "                percentile = trial.suggest_int(\"percentile\", min_percent, max_percent)\n",
    "                selector = SelectPercentile(f_classif, percentile=percentile)\n",
    "            elif feat_method == \"pca\":\n",
    "                n_components = trial.suggest_int(\"pca_n_components\", 10, max_pca)\n",
    "                selector = PCA(n_components=n_components, random_state=42)\n",
    "            else:\n",
    "                n_components = trial.suggest_int(\"ica_n_components\", 10, max_ica)\n",
    "                selector = FastICA(n_components=n_components, random_state=42, max_iter=1000)\n",
    "\n",
    "            # Model selection\n",
    "            if model_type == \"svc\":\n",
    "                kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"sigmoid\"])\n",
    "                C = trial.suggest_float(\"C\", 1e-2, 1e1, log=True)\n",
    "                if kernel == \"linear\":\n",
    "                    model = SVC(kernel=\"linear\", C=C, random_state=42)\n",
    "                else:\n",
    "                    gamma = trial.suggest_float(\"gamma\", 1e-4, 1e-1, log=True)\n",
    "                    coef0 = trial.suggest_float(\"coef0\", 0.0, 1.0)\n",
    "                    degree = trial.suggest_int(\"degree\", 2, 6) if kernel == \"poly\" else 3\n",
    "                    model = SVC(kernel=kernel, C=C, gamma=gamma, coef0=coef0, degree=degree, random_state=42)\n",
    "            elif model_type == \"linear_svc\":\n",
    "                C = trial.suggest_float(\"C\", 1e-3, 1e2, log=True)\n",
    "                model = LinearSVC(C=C, max_iter=10000, random_state=42)\n",
    "            else:\n",
    "                n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "                max_depth = trial.suggest_int(\"max_depth\", 2, 20)\n",
    "                min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 5)\n",
    "                min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 3)\n",
    "                model = RandomForestClassifier(\n",
    "                    n_estimators=n_estimators,\n",
    "                    max_depth=max_depth,\n",
    "                    min_samples_split=min_samples_split,\n",
    "                    min_samples_leaf=min_samples_leaf,\n",
    "                    random_state=42,\n",
    "                )\n",
    "\n",
    "            pipeline = make_pipeline(StandardScaler(), selector, model)\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "            scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring=\"f1_macro\", n_jobs=1)\n",
    "            return scores.mean()\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective_wrapper, n_trials=n_trials)\n",
    "        best_params = study.best_params\n",
    "        print(f\"Best Params for {model_type} = {best_params}\")\n",
    "\n",
    "        # Build final pipeline\n",
    "        feat_method = best_params[\"feat_method\"]\n",
    "        if feat_method == \"anova\":\n",
    "            selector = SelectPercentile(f_classif, percentile=best_params[\"percentile\"])\n",
    "        elif feat_method == \"pca\":\n",
    "            selector = PCA(n_components=best_params[\"pca_n_components\"], random_state=42)\n",
    "        else:\n",
    "            selector = FastICA(n_components=best_params[\"ica_n_components\"], random_state=42, max_iter=1000)\n",
    "\n",
    "        if model_type == \"svc\":\n",
    "            if best_params[\"kernel\"] == \"linear\":\n",
    "                model = SVC(kernel=\"linear\", C=best_params[\"C\"], random_state=42)\n",
    "            else:\n",
    "                model = SVC(\n",
    "                    kernel=best_params[\"kernel\"],\n",
    "                    C=best_params[\"C\"],\n",
    "                    gamma=best_params.get(\"gamma\", \"scale\"),\n",
    "                    coef0=best_params.get(\"coef0\", 0),\n",
    "                    degree=best_params.get(\"degree\", 3),\n",
    "                    random_state=42,\n",
    "                )\n",
    "        elif model_type == \"linear_svc\":\n",
    "            model = LinearSVC(C=best_params[\"C\"], max_iter=10000, random_state=42)\n",
    "        else:\n",
    "            model = RandomForestClassifier(\n",
    "                n_estimators=best_params[\"n_estimators\"],\n",
    "                max_depth=best_params[\"max_depth\"],\n",
    "                min_samples_split=best_params[\"min_samples_split\"],\n",
    "                min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "                random_state=42,\n",
    "            )\n",
    "\n",
    "        final_pipeline = make_pipeline(StandardScaler(), selector, model)\n",
    "        final_pipeline.fit(X_train, y_train)\n",
    "        y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        print(f\"FINAL RESULTS {model_type.upper()}: acc={acc:.3f}, f1={f1:.3f}\")\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        ConfusionMatrixDisplay(cm, display_labels=train_dataset.encoder.classes_).plot(cmap=\"Blues\")\n",
    "        plt.show()\n",
    "\n",
    "        all_results[model_type] = {\n",
    "            \"pipeline\": final_pipeline,\n",
    "            \"acc\": acc,\n",
    "            \"f1\": f1,\n",
    "            \"cm\": cm,\n",
    "            \"best_params\": best_params,\n",
    "            \"study\": study,\n",
    "        }\n",
    "\n",
    "    return all_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91383595-b0cb-4f43-842e-0c1f25b82c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_async_aal.eeg = np.zeros_like(train_async_aal.fmri)  # same shape as fmri or at least first dim\n",
    "test_async_aal.eeg  = np.zeros_like(test_async_aal.fmri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf8da7-d9a3-4f27-8f45-f66b0c1ff4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 10:58:21,397] A new study created in memory with name: no-name-6536ea35-aa8d-48f7-a531-9d1b02e7e4a2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ OPTUNA + 10-Fold CV for: SVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 10:58:21,578] Trial 0 finished with value: 0.13748015873015873 and parameters: {'feat_method': 'anova', 'percentile': 51, 'kernel': 'linear', 'C': 0.029234663897664038}. Best is trial 0 with value: 0.13748015873015873.\n",
      "[I 2025-12-11 10:58:21,906] Trial 1 finished with value: 0.030007434774676156 and parameters: {'feat_method': 'anova', 'percentile': 79, 'kernel': 'poly', 'C': 0.06137347207941753, 'gamma': 0.002784914713937487, 'coef0': 0.2811739259665452, 'degree': 6}. Best is trial 0 with value: 0.13748015873015873.\n",
      "[I 2025-12-11 10:58:22,106] Trial 2 finished with value: 0.08575514763014762 and parameters: {'feat_method': 'anova', 'percentile': 4, 'kernel': 'linear', 'C': 2.0887194490206}. Best is trial 0 with value: 0.13748015873015873.\n",
      "[I 2025-12-11 10:58:25,192] Trial 3 finished with value: 0.0899226467976468 and parameters: {'feat_method': 'ica', 'ica_n_components': 52, 'kernel': 'sigmoid', 'C': 0.22928844465022913, 'gamma': 0.009890948889046168, 'coef0': 0.371790359849397}. Best is trial 0 with value: 0.13748015873015873.\n",
      "[I 2025-12-11 10:58:25,408] Trial 4 finished with value: 0.05507648126727074 and parameters: {'feat_method': 'anova', 'percentile': 71, 'kernel': 'sigmoid', 'C': 4.919428330422409, 'gamma': 0.00013230971974767742, 'coef0': 0.06633938218625901}. Best is trial 0 with value: 0.13748015873015873.\n",
      "[I 2025-12-11 10:58:25,570] Trial 5 finished with value: 0.03868456504795513 and parameters: {'feat_method': 'anova', 'percentile': 38, 'kernel': 'poly', 'C': 0.023774303870874498, 'gamma': 0.005974314974836358, 'coef0': 0.8245868982277522, 'degree': 6}. Best is trial 0 with value: 0.13748015873015873.\n",
      "[I 2025-12-11 10:58:25,697] Trial 6 finished with value: 0.05850080066991832 and parameters: {'feat_method': 'anova', 'percentile': 13, 'kernel': 'sigmoid', 'C': 0.6279820176324963, 'gamma': 0.002314482412228708, 'coef0': 0.06489097182503878}. Best is trial 0 with value: 0.13748015873015873.\n",
      "[I 2025-12-11 10:58:25,880] Trial 7 finished with value: 0.03144605075639558 and parameters: {'feat_method': 'anova', 'percentile': 67, 'kernel': 'poly', 'C': 0.4233237384381911, 'gamma': 0.005335414374248611, 'coef0': 0.15838321052073823, 'degree': 6}. Best is trial 0 with value: 0.13748015873015873.\n",
      "[I 2025-12-11 10:58:28,600] Trial 8 finished with value: 0.08825695138195137 and parameters: {'feat_method': 'ica', 'ica_n_components': 57, 'kernel': 'sigmoid', 'C': 0.037724727373619336, 'gamma': 0.0003864994934772418, 'coef0': 0.9247253365930174}. Best is trial 0 with value: 0.13748015873015873.\n",
      "[I 2025-12-11 10:58:28,915] Trial 9 finished with value: 0.1486111111111111 and parameters: {'feat_method': 'pca', 'pca_n_components': 85, 'kernel': 'linear', 'C': 0.04023745765321003}. Best is trial 9 with value: 0.1486111111111111.\n",
      "[I 2025-12-11 10:58:29,277] Trial 10 finished with value: 0.13363455988455988 and parameters: {'feat_method': 'pca', 'pca_n_components': 87, 'kernel': 'linear', 'C': 0.11981659031078232}. Best is trial 9 with value: 0.1486111111111111.\n",
      "[I 2025-12-11 10:58:29,535] Trial 11 finished with value: 0.11056637806637806 and parameters: {'feat_method': 'pca', 'pca_n_components': 36, 'kernel': 'linear', 'C': 0.010102741013669805}. Best is trial 9 with value: 0.1486111111111111.\n",
      "[I 2025-12-11 10:58:29,869] Trial 12 finished with value: 0.148612012987013 and parameters: {'feat_method': 'pca', 'pca_n_components': 116, 'kernel': 'linear', 'C': 0.012901378062986742}. Best is trial 12 with value: 0.148612012987013.\n",
      "[I 2025-12-11 10:58:30,202] Trial 13 finished with value: 0.1354265873015873 and parameters: {'feat_method': 'pca', 'pca_n_components': 116, 'kernel': 'linear', 'C': 0.01225141994723689}. Best is trial 12 with value: 0.148612012987013.\n",
      "[I 2025-12-11 10:58:30,585] Trial 14 finished with value: 0.13494227994227995 and parameters: {'feat_method': 'pca', 'pca_n_components': 116, 'kernel': 'linear', 'C': 0.09380481593130593}. Best is trial 12 with value: 0.148612012987013.\n",
      "[I 2025-12-11 10:58:30,871] Trial 15 finished with value: 0.15280871905871907 and parameters: {'feat_method': 'pca', 'pca_n_components': 73, 'kernel': 'linear', 'C': 0.020631125111631333}. Best is trial 15 with value: 0.15280871905871907.\n",
      "[I 2025-12-11 10:58:31,096] Trial 16 finished with value: 0.07750343406593407 and parameters: {'feat_method': 'pca', 'pca_n_components': 10, 'kernel': 'linear', 'C': 0.017246085009153662}. Best is trial 15 with value: 0.15280871905871907.\n",
      "[I 2025-12-11 10:58:31,476] Trial 17 finished with value: 0.12475378787878788 and parameters: {'feat_method': 'pca', 'pca_n_components': 62, 'kernel': 'linear', 'C': 1.0345083070713725}. Best is trial 15 with value: 0.15280871905871907.\n",
      "[I 2025-12-11 10:58:31,837] Trial 18 finished with value: 0.1269381313131313 and parameters: {'feat_method': 'pca', 'pca_n_components': 82, 'kernel': 'linear', 'C': 0.18727905668461253}. Best is trial 15 with value: 0.15280871905871907.\n",
      "[I 2025-12-11 10:58:37,292] Trial 19 finished with value: 0.11516192141192141 and parameters: {'feat_method': 'ica', 'ica_n_components': 13, 'kernel': 'poly', 'C': 9.84876510368139, 'gamma': 0.07330800935042325, 'coef0': 0.6595941444401041, 'degree': 2}. Best is trial 15 with value: 0.15280871905871907.\n",
      "[I 2025-12-11 10:58:37,727] Trial 20 finished with value: 0.16502435064935064 and parameters: {'feat_method': 'pca', 'pca_n_components': 55, 'kernel': 'linear', 'C': 0.07244075108144704}. Best is trial 20 with value: 0.16502435064935064.\n"
     ]
    }
   ],
   "source": [
    "results = run_optuna_with_bimodal_dataset(\n",
    "    train_async_aal, \n",
    "    test_async_aal, \n",
    "    n_trials=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba5120d9-09e5-4398-9a37-ea4d702fe1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROI1DCNN(nn.Module):\n",
    "    # Input is (B, L) where L=N_regions. We add C=1 channel dim.\n",
    "    def __init__(self, in_features, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, N_regions) -> adds channel dim (batch, 1, N_regions)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, n_regions, num_classes=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        # FIX: Change kernel_size to 1 since input length L is 1\n",
    "        self.conv1 = nn.Conv1d(n_regions, 64, kernel_size=1) \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=1) # Subsequent convolutions must also be 1\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1) # This is fine\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, N_regions) -> reshape to (batch, N_regions, 1) to fit Conv1D structure\n",
    "        # This treats N_regions as channels and sequence length as 1\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # Input is (B, N_regions). Flattening is trivial.\n",
    "    def __init__(self, input_size, num_classes=8, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # (B, N_regions)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65a75c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=1000, LR_DECAY_FACTOR=0.3, LR_PATIENCE=30, MAX_PATIENCE=60):\n",
    "    \"\"\"\n",
    "    Trains the 3D ResNet-50 model with a custom learning rate scheduler \n",
    "    and early stopping based on validation loss.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    # Scheduler: Monitors validation loss and decreases LR when loss doesn't improve\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=LR_DECAY_FACTOR, \n",
    "        patience=LR_PATIENCE, \n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            # Data from BimodalDataset: fMRI, EEG (placeholder), Label\n",
    "            fmri = batch['fmri'].to(device) # Add Channel dim: (B, 1, D, H, W)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(fmri)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * fmri.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        \n",
    "        # Validation Step\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}', end = \"\\r\", flush = True)\n",
    "\n",
    "        # Step LR Scheduler and Check for Early Stopping\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Optionally save the best model weights here\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= MAX_PATIENCE:\n",
    "                #print(f\"Early stopping triggered after {patience_counter} epochs without improvement in validation loss.\")\n",
    "                break\n",
    "                \n",
    "    #print(\"Training finished.\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model's loss and accuracy on a given dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            fmri = batch['fmri'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(fmri)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * fmri.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aabc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Final simplified Dataset: Assumes input 'rois' is the fully processed\n",
    "    (scaled/PCA'd) NumPy array ready for tensor conversion.\n",
    "    \"\"\"\n",
    "    def __init__(self, rois, labels):\n",
    "        self.labels = labels\n",
    "        # rois is now the FINAL (N_samples, N_features_final) NumPy array\n",
    "        self.rois = rois\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rois)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # x is the final feature vector, shape (N_features_final,)\n",
    "        x = self.rois[idx].astype(np.float32)\n",
    "        \n",
    "        # Convert to tensor. No more scaling or PCA logic here.\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return {\"fmri\": x, \"label\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ceaca1a-f840-49bc-8933-b4ec27859cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(model_name, num_classes, in_channels):\n",
    "    # ... (Model initialization logic)\n",
    "    if model_name == \"ROI1DCNN\":\n",
    "        return ROI1DCNN(in_features = in_channels, num_classes=num_classes)\n",
    "    elif model_name == \"MLP\":\n",
    "        return MLP(input_size=in_channels, num_classes=num_classes)\n",
    "    elif model_name == \"1DCNN\":\n",
    "        return CNN1D(n_regions=in_channels, num_classes=num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model's loss and accuracy on a given dataset.\n",
    "    \"\"\"\n",
    "    # ... (Evaluation logic is unchanged and correct)\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            fmri = batch['fmri'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(fmri)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * fmri.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(data_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=1000, LR_DECAY_FACTOR=0.3, LR_PATIENCE=30, MAX_PATIENCE=60):\n",
    "    \"\"\"\n",
    "    Trains the model with gradient clipping and early stopping.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Scheduler: Monitors validation loss and decreases LR\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=LR_DECAY_FACTOR, patience=LR_PATIENCE\n",
    "    )\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            fmri = batch['fmri'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(fmri)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # --- GRADIENT CLIPPING FIX FOR NAN LOSS ---\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            # ------------------------------------------\n",
    "            \n",
    "            running_loss += loss.item() * fmri.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        \n",
    "        # Validation Step\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}', end = \"\\r\", flush = True)\n",
    "\n",
    "        # Step LR Scheduler and Check for Early Stopping\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss or val_accuracy >= best_val_acc:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc = val_accuracy\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= MAX_PATIENCE:\n",
    "                break\n",
    "                \n",
    "    return model\n",
    "\n",
    "class ROI1DCNN(nn.Module):\n",
    "    def __init__(self, in_features, num_classes, dropout_rate=0.3, n_filters_start=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, n_filters_start, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(n_filters_start, n_filters_start * 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(n_filters_start * 2, n_filters_start * 4, kernel_size=3, padding=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout_rate) # New dropout layer\n",
    "        self.fc = nn.Linear(n_filters_start * 4, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.dropout(x) # Apply dropout after pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, n_regions, num_classes=8, dropout_rate=0.3, n_filters_start=64):\n",
    "        super().__init__()\n",
    "        # Kernel size 1 fixed for L=1 input\n",
    "        self.conv1 = nn.Conv1d(n_regions, n_filters_start, kernel_size=1) \n",
    "        self.conv2 = nn.Conv1d(n_filters_start, n_filters_start * 2, kernel_size=1) \n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(n_filters_start * 2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(-1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=8, dropout_rate=0.3, n_fc_units=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, n_fc_units)\n",
    "        self.fc2 = nn.Linear(n_fc_units, n_fc_units // 2)\n",
    "        self.fc3 = nn.Linear(n_fc_units // 2, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "def get_model(model_name, num_classes, in_channels, **kwargs):\n",
    "    if model_name == \"ROI1DCNN\":\n",
    "        return ROI1DCNN(in_features=in_channels, num_classes=num_classes, **kwargs)\n",
    "    elif model_name == \"MLP\":\n",
    "        return MLP(input_size=in_channels, num_classes=num_classes, **kwargs)\n",
    "    elif model_name == \"1DCNN\":\n",
    "        return CNN1D(n_regions=in_channels, num_classes=num_classes, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "\n",
    "class DummyDatasetContainer:\n",
    "    def __init__(self, n_samples=100, n_regions=50, n_classes=8):\n",
    "        self.fmri = np.random.randn(n_samples, n_regions).astype(np.float32)\n",
    "        self.labels = np.random.randint(0, n_classes, n_samples)\n",
    "        self.label_map = {i: f\"class_{i}\" for i in range(n_classes)}\n",
    "    def inverse_labels(self, labels):\n",
    "        return [self.label_map[l] for l in labels]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1896da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stratified Data Split Complete ---\n",
      "Total samples: 272\n",
      "Training samples: 231 (85.0%)\n",
      "Validation samples: 41 (15.0%)\n",
      "Input dimension (N_regions) fixed at: 116\n",
      "N_CLASSES fixed at: 8\n"
     ]
    }
   ],
   "source": [
    "X_data_raw = train_async_aal.fmri \n",
    "y_cat = train_async_aal.inverse_labels(train_async_aal.labels)\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(y_cat)\n",
    "N_CLASSES = len(le.classes_)\n",
    "\n",
    "# 2. Split ONCE using Stratification (FIXED)\n",
    "TEST_SIZE_RATIO = 0.15 # Using your original 85/15 split ratio\n",
    "\n",
    "# Use a fixed random_state (e.g., 42) to ensure the split is reproducible across entire Optuna runs.\n",
    "# The 'stratify=y_int' argument ensures the class proportions are maintained in both splits.\n",
    "X_TRAIN_RAW, X_VAL_RAW, Y_TRAIN, Y_VAL = train_test_split(\n",
    "    X_data_raw, \n",
    "    y_int, \n",
    "    test_size=TEST_SIZE_RATIO, \n",
    "    random_state=42, # Ensure reproducibility\n",
    "    stratify=y_int   # ENSURES STRATIFICATION\n",
    ")\n",
    "\n",
    "# 3. Fit StandardScaler ONCE\n",
    "# Always fit the scaler only on the training data.\n",
    "SCALER = StandardScaler().fit(X_TRAIN_RAW)\n",
    "TRAIN_MEAN = SCALER.mean_\n",
    "TRAIN_STD = np.sqrt(SCALER.var_)\n",
    "INPUT_CHANNELS_RAW = X_TRAIN_RAW.shape[1]\n",
    "\n",
    "print(\"--- Stratified Data Split Complete ---\")\n",
    "print(f\"Total samples: {len(X_data_raw)}\")\n",
    "print(f\"Training samples: {len(X_TRAIN_RAW)} ({100 - (TEST_SIZE_RATIO * 100)}%)\")\n",
    "print(f\"Validation samples: {len(X_VAL_RAW)} ({TEST_SIZE_RATIO * 100}%)\")\n",
    "print(f\"Input dimension (N_regions) fixed at: {INPUT_CHANNELS_RAW}\")\n",
    "print(f\"N_CLASSES fixed at: {N_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72a74ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Class Distribution (Train vs. Validation) ---\n",
      "          Train Count Train Percentage  Validation Count Validation Percentage\n",
      "Class ID                                                                      \n",
      "0                  29           12.55%                 5                12.20%\n",
      "1                  29           12.55%                 5                12.20%\n",
      "2                  29           12.55%                 5                12.20%\n",
      "3                  29           12.55%                 5                12.20%\n",
      "4                  29           12.55%                 5                12.20%\n",
      "5                  29           12.55%                 5                12.20%\n",
      "6                  28           12.12%                 6                14.63%\n",
      "7                  29           12.55%                 5                12.20%\n",
      "--------------------------------------------------\n",
      "Total Samples in Training Set: 231\n",
      "Total Samples in Validation Set: 41\n"
     ]
    }
   ],
   "source": [
    "# --- Analysis of Training Set ---\n",
    "from collections import Counter\n",
    "train_counts = Counter(Y_TRAIN)\n",
    "total_train = len(Y_TRAIN)\n",
    "train_data = []\n",
    "\n",
    "for class_id in range(N_CLASSES):\n",
    "    count = train_counts.get(class_id, 0)\n",
    "    percentage = (count / total_train) * 100 if total_train > 0 else 0\n",
    "    train_data.append({\n",
    "        'Class ID': class_id,\n",
    "        'Train Count': count,\n",
    "        'Train Percentage': f'{percentage:.2f}%'\n",
    "    })\n",
    "\n",
    "df_train = pd.DataFrame(train_data).set_index('Class ID')\n",
    "\n",
    "# --- Analysis of Validation Set ---\n",
    "val_counts = Counter(Y_VAL)\n",
    "total_val = len(Y_VAL)\n",
    "val_data = []\n",
    "\n",
    "for class_id in range(N_CLASSES):\n",
    "    count = val_counts.get(class_id, 0)\n",
    "    percentage = (count / total_val) * 100 if total_val > 0 else 0\n",
    "    val_data.append({\n",
    "        'Class ID': class_id,\n",
    "        'Validation Count': count,\n",
    "        'Validation Percentage': f'{percentage:.2f}%'\n",
    "    })\n",
    "\n",
    "df_val = pd.DataFrame(val_data).set_index('Class ID')\n",
    "\n",
    "# --- Combine and Print Results ---\n",
    "df_combined = df_train.join(df_val)\n",
    "\n",
    "print(\"--- Class Distribution (Train vs. Validation) ---\")\n",
    "print(df_combined)\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total Samples in Training Set: {total_train}\")\n",
    "print(f\"Total Samples in Validation Set: {total_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "597574a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # --- 1. General Hyperparameters ---\n",
    "    \n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32, 64])\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-3) \n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-7, 1e-3)\n",
    "    dropout_rate = trial.suggest_uniform(\"dropout_rate\", 0.1, 0.7)\n",
    "    lr_decay_factor = trial.suggest_uniform(\"lr_decay_factor\", 0.1, 0.9)\n",
    "    lr_patience = trial.suggest_int(\"lr_patience\", 5, 50)\n",
    "    max_patience = trial.suggest_int(\"max_patience\", 20, 100)\n",
    "    model_type = trial.suggest_categorical(\"model_type\", [\"ROI1DCNN\", \"MLP\", \"1DCNN\"])\n",
    "    \n",
    "    # --- 2. Model-Specific Hyperparameters ---\n",
    "    \n",
    "    model_kwargs = {\"dropout_rate\": dropout_rate}\n",
    "    if model_type == \"ROI1DCNN\":\n",
    "        model_kwargs[\"n_filters_start\"] = trial.suggest_categorical(\"roi_n_filters_start\", [8, 16, 32, 64, 128])\n",
    "    elif model_type == \"1DCNN\":\n",
    "        model_kwargs[\"n_filters_start\"] = trial.suggest_categorical(\"cnn_n_filters_start\", [16, 32, 64, 128])\n",
    "    elif model_type == \"MLP\":\n",
    "        model_kwargs[\"n_fc_units\"] = trial.suggest_categorical(\"mlp_n_fc_units\", [16, 32, 64, 128, 256])\n",
    "        \n",
    "    # --- 3. Feature Engineering (Standardize -> PCA) ---\n",
    "    \n",
    "    # Apply Standardization using the globally fitted mean/std\n",
    "    X_train_scaled = (X_TRAIN_RAW - TRAIN_MEAN) / (TRAIN_STD + 1e-8)\n",
    "    X_val_scaled = (X_VAL_RAW - TRAIN_MEAN) / (TRAIN_STD + 1e-8)\n",
    "    \n",
    "    # Set final data arrays to the scaled data initially\n",
    "    X_train_final = X_train_scaled\n",
    "    X_val_final = X_val_scaled\n",
    "    in_ch = INPUT_CHANNELS_RAW\n",
    "    pca = None\n",
    "\n",
    "    use_pca = trial.suggest_categorical(\"use_pca\", [False, True])\n",
    "    \n",
    "    if use_pca:\n",
    "        # PCA Fit (on scaled data)\n",
    "        # Note: We can suggest up to the number of original features\n",
    "        n_components = trial.suggest_int(\"n_components\", 2, INPUT_CHANNELS_RAW)\n",
    "        pca = PCA(n_components=n_components).fit(X_train_scaled)\n",
    "        \n",
    "        # Transform\n",
    "        X_train_final = pca.transform(X_train_scaled)\n",
    "        X_val_final = pca.transform(X_val_scaled)\n",
    "        \n",
    "        # Update Model Input size\n",
    "        in_ch = pca.n_components_\n",
    "    \n",
    "    # --- 4. Data Loaders ---\n",
    "    \n",
    "    # ROIDataset now receives the FINAL processed data.\n",
    "    train_loader = DataLoader(\n",
    "        ROIDataset(X_train_final, Y_TRAIN),\n",
    "        batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        ROIDataset(X_val_final, Y_VAL),\n",
    "        batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    # --- 5. Model Training and Evaluation ---\n",
    "    \n",
    "    num_classes = N_CLASSES # Use the globally defined number of classes\n",
    "    \n",
    "    model = get_model(model_type, num_classes=num_classes, in_channels=in_ch, **model_kwargs)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Train & evaluate \n",
    "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer,\n",
    "                                num_epochs=1000, LR_DECAY_FACTOR=lr_decay_factor,\n",
    "                                LR_PATIENCE=lr_patience, MAX_PATIENCE=max_patience)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    val_loss, val_acc = evaluate_model(trained_model, val_loader, criterion, device)\n",
    "\n",
    "    return val_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74430173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 21:51:50,202] A new study created in memory with name: no-name-41290562-3cf9-4452-85db-b179a59f6bc3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0760 | Train Acc: 0.1515 | Val Loss: 2.0848 | Val Acc: 0.1463\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 21:53:54,348] Trial 0 finished with value: 2.084769539716767 and parameters: {'batch_size': 8, 'learning_rate': 1.3225391290702208e-05, 'weight_decay': 1.3190622108677502e-05, 'dropout_rate': 0.5774054896007377, 'lr_decay_factor': 0.4965300303078065, 'lr_patience': 35, 'max_patience': 88, 'model_type': 'MLP', 'mlp_n_fc_units': 16, 'use_pca': False}. Best is trial 0 with value: 2.084769539716767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/1000 | Train Loss: 2.0604 | Train Acc: 0.1645 | Val Loss: 2.0911 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 21:54:06,762] Trial 1 finished with value: 2.0911407121797887 and parameters: {'batch_size': 8, 'learning_rate': 1.058926900230194e-06, 'weight_decay': 0.0003833007230473852, 'dropout_rate': 0.27462136722194147, 'lr_decay_factor': 0.5132111420159794, 'lr_patience': 35, 'max_patience': 75, 'model_type': 'ROI1DCNN', 'roi_n_filters_start': 64, 'use_pca': True, 'n_components': 25}. Best is trial 0 with value: 2.084769539716767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/1000 | Train Loss: 1.9684 | Train Acc: 0.2641 | Val Loss: 2.1425 | Val Acc: 0.2195\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 21:54:13,140] Trial 2 finished with value: 2.142548002847811 and parameters: {'batch_size': 16, 'learning_rate': 0.00019222708282375103, 'weight_decay': 4.6830946190038084e-06, 'dropout_rate': 0.5429523527834272, 'lr_decay_factor': 0.10804375695359987, 'lr_patience': 31, 'max_patience': 77, 'model_type': '1DCNN', 'cnn_n_filters_start': 32, 'use_pca': False}. Best is trial 0 with value: 2.084769539716767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0715 | Train Acc: 0.1385 | Val Loss: 2.0999 | Val Acc: 0.0732\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 21:55:07,731] Trial 3 finished with value: 2.0998519629966923 and parameters: {'batch_size': 32, 'learning_rate': 1.0398890564437236e-05, 'weight_decay': 3.554206052867983e-06, 'dropout_rate': 0.4163086267905328, 'lr_decay_factor': 0.22150236638555, 'lr_patience': 43, 'max_patience': 79, 'model_type': 'ROI1DCNN', 'roi_n_filters_start': 32, 'use_pca': True, 'n_components': 22}. Best is trial 0 with value: 2.084769539716767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 0.5562 | Train Acc: 0.6840 | Val Loss: 5.2247 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 21:57:20,866] Trial 4 finished with value: 5.224745389891834 and parameters: {'batch_size': 8, 'learning_rate': 0.0003641802761074979, 'weight_decay': 2.577564365959445e-07, 'dropout_rate': 0.2246973541493988, 'lr_decay_factor': 0.5297152828941164, 'lr_patience': 36, 'max_patience': 61, 'model_type': '1DCNN', 'cnn_n_filters_start': 64, 'use_pca': True, 'n_components': 63}. Best is trial 0 with value: 2.084769539716767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/1000 | Train Loss: 1.8880 | Train Acc: 0.3636 | Val Loss: 2.1724 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 21:57:39,472] Trial 5 finished with value: 2.17238039505191 and parameters: {'batch_size': 16, 'learning_rate': 8.501368773702027e-05, 'weight_decay': 0.0002668398036877116, 'dropout_rate': 0.6136015869733055, 'lr_decay_factor': 0.8757635361573199, 'lr_patience': 34, 'max_patience': 78, 'model_type': '1DCNN', 'cnn_n_filters_start': 32, 'use_pca': True, 'n_components': 43}. Best is trial 0 with value: 2.084769539716767.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0842 | Train Acc: 0.1082 | Val Loss: 2.0773 | Val Acc: 0.1707\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:02:39,663] Trial 6 finished with value: 2.0772791548473077 and parameters: {'batch_size': 4, 'learning_rate': 1.6213091870462766e-06, 'weight_decay': 0.000597856699338558, 'dropout_rate': 0.3658191378815091, 'lr_decay_factor': 0.34847612341704615, 'lr_patience': 45, 'max_patience': 33, 'model_type': 'ROI1DCNN', 'roi_n_filters_start': 16, 'use_pca': False}. Best is trial 6 with value: 2.0772791548473077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0736 | Train Acc: 0.1429 | Val Loss: 2.0867 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:03:18,123] Trial 7 finished with value: 2.086724455763654 and parameters: {'batch_size': 32, 'learning_rate': 1.407639530825737e-05, 'weight_decay': 3.1082313410171903e-06, 'dropout_rate': 0.5297321198917267, 'lr_decay_factor': 0.5195216549869008, 'lr_patience': 12, 'max_patience': 64, 'model_type': '1DCNN', 'cnn_n_filters_start': 16, 'use_pca': False}. Best is trial 6 with value: 2.0772791548473077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 0.5071 | Train Acc: 0.6623 | Val Loss: 4.6865 | Val Acc: 0.1707\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:04:21,999] Trial 8 finished with value: 4.686517006013451 and parameters: {'batch_size': 16, 'learning_rate': 0.00030251229777611936, 'weight_decay': 0.0001956750434411149, 'dropout_rate': 0.1606497719106886, 'lr_decay_factor': 0.44551628668209065, 'lr_patience': 49, 'max_patience': 80, 'model_type': '1DCNN', 'cnn_n_filters_start': 128, 'use_pca': False}. Best is trial 6 with value: 2.0772791548473077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 1.1867 | Train Acc: 0.5758 | Val Loss: 2.9504 | Val Acc: 0.1463\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:05:32,278] Trial 9 finished with value: 2.9503907459538157 and parameters: {'batch_size': 16, 'learning_rate': 6.047103653877727e-05, 'weight_decay': 8.53229827794501e-05, 'dropout_rate': 0.2905674525942914, 'lr_decay_factor': 0.5583027624824298, 'lr_patience': 39, 'max_patience': 40, 'model_type': 'ROI1DCNN', 'roi_n_filters_start': 128, 'use_pca': True, 'n_components': 72}. Best is trial 6 with value: 2.0772791548473077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.1032 | Train Acc: 0.1472 | Val Loss: 2.0827 | Val Acc: 0.0732\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:08:54,185] Trial 10 finished with value: 2.082659052639473 and parameters: {'batch_size': 4, 'learning_rate': 1.360634449214989e-06, 'weight_decay': 0.000865812400337987, 'dropout_rate': 0.39392082773237586, 'lr_decay_factor': 0.3180487199375168, 'lr_patience': 17, 'max_patience': 22, 'model_type': 'MLP', 'mlp_n_fc_units': 128, 'use_pca': False}. Best is trial 6 with value: 2.0772791548473077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0835 | Train Acc: 0.1385 | Val Loss: 2.1112 | Val Acc: 0.1463\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:12:12,278] Trial 11 finished with value: 2.1111999139553164 and parameters: {'batch_size': 4, 'learning_rate': 1.1823735267093768e-06, 'weight_decay': 0.0009298099058307266, 'dropout_rate': 0.4025437874002135, 'lr_decay_factor': 0.3196302746099355, 'lr_patience': 18, 'max_patience': 21, 'model_type': 'MLP', 'mlp_n_fc_units': 128, 'use_pca': False}. Best is trial 6 with value: 2.0772791548473077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0768 | Train Acc: 0.1212 | Val Loss: 2.0880 | Val Acc: 0.1951\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:15:30,788] Trial 12 finished with value: 2.0880125499353177 and parameters: {'batch_size': 4, 'learning_rate': 3.243076640810157e-06, 'weight_decay': 4.094761713053124e-05, 'dropout_rate': 0.3606957035552464, 'lr_decay_factor': 0.3270516497666379, 'lr_patience': 22, 'max_patience': 20, 'model_type': 'MLP', 'mlp_n_fc_units': 128, 'use_pca': False}. Best is trial 6 with value: 2.0772791548473077.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0977 | Train Acc: 0.1039 | Val Loss: 2.0751 | Val Acc: 0.1463\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:15:52,430] Trial 13 finished with value: 2.0750961303710938 and parameters: {'batch_size': 64, 'learning_rate': 3.852489180727263e-06, 'weight_decay': 0.0008913514217222117, 'dropout_rate': 0.4662255213425802, 'lr_decay_factor': 0.6722840239941166, 'lr_patience': 7, 'max_patience': 36, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 13 with value: 2.0750961303710938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0818 | Train Acc: 0.1299 | Val Loss: 2.0792 | Val Acc: 0.1707\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:16:21,237] Trial 14 finished with value: 2.079191207885742 and parameters: {'batch_size': 64, 'learning_rate': 3.995392868898899e-06, 'weight_decay': 5.03600541208108e-05, 'dropout_rate': 0.4758413100373214, 'lr_decay_factor': 0.7089233619392589, 'lr_patience': 5, 'max_patience': 40, 'model_type': 'ROI1DCNN', 'roi_n_filters_start': 16, 'use_pca': False}. Best is trial 13 with value: 2.0750961303710938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.1258 | Train Acc: 0.1126 | Val Loss: 2.0802 | Val Acc: 0.2195\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:16:42,792] Trial 15 finished with value: 2.0801587104797363 and parameters: {'batch_size': 64, 'learning_rate': 4.171131839450803e-06, 'weight_decay': 4.4473063852874705e-07, 'dropout_rate': 0.6938320828912303, 'lr_decay_factor': 0.6703520199682383, 'lr_patience': 27, 'max_patience': 39, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 13 with value: 2.0750961303710938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/1000 | Train Loss: 1.4754 | Train Acc: 0.4848 | Val Loss: 2.5744 | Val Acc: 0.0976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:16:46,528] Trial 16 finished with value: 2.5744030475616455 and parameters: {'batch_size': 64, 'learning_rate': 0.0009624039605021209, 'weight_decay': 1.695222844636561e-05, 'dropout_rate': 0.3232585913214464, 'lr_decay_factor': 0.7741478321820809, 'lr_patience': 50, 'max_patience': 49, 'model_type': 'ROI1DCNN', 'roi_n_filters_start': 16, 'use_pca': False}. Best is trial 13 with value: 2.0750961303710938.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0819 | Train Acc: 0.1126 | Val Loss: 2.0706 | Val Acc: 0.1707\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:20:03,028] Trial 17 finished with value: 2.0705924324873015 and parameters: {'batch_size': 4, 'learning_rate': 2.5790876492928337e-06, 'weight_decay': 0.00012872441203657222, 'dropout_rate': 0.47382462212579046, 'lr_decay_factor': 0.41697914851502504, 'lr_patience': 5, 'max_patience': 30, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 17 with value: 2.0705924324873015.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/1000 | Train Loss: 2.0594 | Train Acc: 0.1861 | Val Loss: 2.0809 | Val Acc: 0.0732\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:20:04,340] Trial 18 finished with value: 2.0808823108673096 and parameters: {'batch_size': 64, 'learning_rate': 2.7511897743259826e-05, 'weight_decay': 0.00012830684222522255, 'dropout_rate': 0.4692756048604667, 'lr_decay_factor': 0.6545431870628201, 'lr_patience': 5, 'max_patience': 54, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 17 with value: 2.0705924324873015.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/1000 | Train Loss: 2.1101 | Train Acc: 0.1212 | Val Loss: 2.0874 | Val Acc: 0.0976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:20:19,815] Trial 19 finished with value: 2.0873768620374724 and parameters: {'batch_size': 4, 'learning_rate': 8.178793862022142e-06, 'weight_decay': 3.315647014351398e-05, 'dropout_rate': 0.65133461204248, 'lr_decay_factor': 0.8869132155986137, 'lr_patience': 11, 'max_patience': 29, 'model_type': 'MLP', 'mlp_n_fc_units': 64, 'use_pca': False}. Best is trial 17 with value: 2.0705924324873015.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0978 | Train Acc: 0.1169 | Val Loss: 2.0861 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:20:41,137] Trial 20 finished with value: 2.0861129760742188 and parameters: {'batch_size': 64, 'learning_rate': 2.7982482778832464e-06, 'weight_decay': 8.142165542242897e-07, 'dropout_rate': 0.45868645999962165, 'lr_decay_factor': 0.40431120410440113, 'lr_patience': 10, 'max_patience': 48, 'model_type': 'MLP', 'mlp_n_fc_units': 32, 'use_pca': False}. Best is trial 17 with value: 2.0705924324873015.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0744 | Train Acc: 0.1299 | Val Loss: 2.0877 | Val Acc: 0.1463\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:25:43,768] Trial 21 finished with value: 2.0876598852436716 and parameters: {'batch_size': 4, 'learning_rate': 2.294998341743475e-06, 'weight_decay': 0.0004160814591869095, 'dropout_rate': 0.5112113315432991, 'lr_decay_factor': 0.22352823928911508, 'lr_patience': 26, 'max_patience': 31, 'model_type': 'ROI1DCNN', 'roi_n_filters_start': 8, 'use_pca': False}. Best is trial 17 with value: 2.0705924324873015.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/1000 | Train Loss: 2.0754 | Train Acc: 0.1342 | Val Loss: 2.0542 | Val Acc: 0.1951\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:26:07,658] Trial 22 finished with value: 2.054228852434856 and parameters: {'batch_size': 4, 'learning_rate': 5.718377265540766e-06, 'weight_decay': 0.000960325743686248, 'dropout_rate': 0.3332401875365453, 'lr_decay_factor': 0.6046018524795926, 'lr_patience': 16, 'max_patience': 31, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 1.9838 | Train Acc: 0.3766 | Val Loss: 2.1045 | Val Acc: 0.1463\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:30:26,156] Trial 23 finished with value: 2.1044543138364467 and parameters: {'batch_size': 4, 'learning_rate': 6.574236425093657e-06, 'weight_decay': 0.0001413309061302247, 'dropout_rate': 0.10569036427286305, 'lr_decay_factor': 0.6088959479857214, 'lr_patience': 15, 'max_patience': 29, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 1.9847 | Train Acc: 0.3377 | Val Loss: 2.0769 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:34:51,474] Trial 24 finished with value: 2.0768750935066036 and parameters: {'batch_size': 4, 'learning_rate': 1.8651159089930162e-05, 'weight_decay': 0.00029012865281115893, 'dropout_rate': 0.23237665106374564, 'lr_decay_factor': 0.7158322080412881, 'lr_patience': 8, 'max_patience': 46, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0576 | Train Acc: 0.1299 | Val Loss: 2.0929 | Val Acc: 0.1463\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:35:41,898] Trial 25 finished with value: 2.092949431116988 and parameters: {'batch_size': 32, 'learning_rate': 5.555197789938541e-06, 'weight_decay': 0.0009951341849973089, 'dropout_rate': 0.4370496951030009, 'lr_decay_factor': 0.7945100051731104, 'lr_patience': 21, 'max_patience': 36, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0734 | Train Acc: 0.1299 | Val Loss: 2.0883 | Val Acc: 0.0976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:36:08,626] Trial 26 finished with value: 2.0882527828216553 and parameters: {'batch_size': 64, 'learning_rate': 2.1149662924374635e-06, 'weight_decay': 8.071696650733322e-05, 'dropout_rate': 0.33769972562017736, 'lr_decay_factor': 0.6001845517342281, 'lr_patience': 13, 'max_patience': 26, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': True, 'n_components': 113}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0780 | Train Acc: 0.1602 | Val Loss: 2.0991 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:43:55,467] Trial 27 finished with value: 2.0990989208221436 and parameters: {'batch_size': 4, 'learning_rate': 2.0713352725648663e-05, 'weight_decay': 0.0004075854657386073, 'dropout_rate': 0.5001681838931002, 'lr_decay_factor': 0.4452751111612899, 'lr_patience': 8, 'max_patience': 68, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/1000 | Train Loss: 2.0566 | Train Acc: 0.1688 | Val Loss: 2.0849 | Val Acc: 0.1463\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:44:42,811] Trial 28 finished with value: 2.0848670878061433 and parameters: {'batch_size': 4, 'learning_rate': 5.5075190243655794e-05, 'weight_decay': 0.0002003232640907942, 'dropout_rate': 0.5732523695192809, 'lr_decay_factor': 0.6102301365849061, 'lr_patience': 8, 'max_patience': 100, 'model_type': 'MLP', 'mlp_n_fc_units': 64, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.1062 | Train Acc: 0.0996 | Val Loss: 2.0950 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:48:02,348] Trial 29 finished with value: 2.094997720020573 and parameters: {'batch_size': 8, 'learning_rate': 5.302412338325724e-06, 'weight_decay': 1.6564454304740534e-05, 'dropout_rate': 0.5525418643311314, 'lr_decay_factor': 0.7839308672514573, 'lr_patience': 20, 'max_patience': 45, 'model_type': 'MLP', 'mlp_n_fc_units': 16, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.1057 | Train Acc: 0.1039 | Val Loss: 2.0963 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:48:43,894] Trial 30 finished with value: 2.096273183822632 and parameters: {'batch_size': 64, 'learning_rate': 1.0140122963866422e-05, 'weight_decay': 0.0005743348573924996, 'dropout_rate': 0.6142816196446085, 'lr_decay_factor': 0.44514609845559366, 'lr_patience': 15, 'max_patience': 26, 'model_type': 'MLP', 'mlp_n_fc_units': 32, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000 | Train Loss: 1.9971 | Train Acc: 0.3247 | Val Loss: 2.1026 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:49:16,306] Trial 31 finished with value: 2.1025942302331693 and parameters: {'batch_size': 4, 'learning_rate': 1.915247580616157e-05, 'weight_decay': 0.0002926789135283355, 'dropout_rate': 0.24239681482970826, 'lr_decay_factor': 0.7277608822640884, 'lr_patience': 8, 'max_patience': 53, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0230 | Train Acc: 0.2554 | Val Loss: 2.0899 | Val Acc: 0.2195\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 22:57:11,011] Trial 32 finished with value: 2.089931709010427 and parameters: {'batch_size': 4, 'learning_rate': 1.4621915393569575e-05, 'weight_decay': 0.0005289991368067277, 'dropout_rate': 0.20622413948635915, 'lr_decay_factor': 0.6721706725522791, 'lr_patience': 5, 'max_patience': 42, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 1.8686 | Train Acc: 0.4892 | Val Loss: 2.1097 | Val Acc: 0.1951\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 23:06:23,298] Trial 33 finished with value: 2.109693213206966 and parameters: {'batch_size': 4, 'learning_rate': 3.451542875453364e-05, 'weight_decay': 9.640391532376866e-05, 'dropout_rate': 0.28159300853259056, 'lr_decay_factor': 0.7310693962289193, 'lr_patience': 9, 'max_patience': 35, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/1000 | Train Loss: 2.0349 | Train Acc: 0.2251 | Val Loss: 2.1025 | Val Acc: 0.0488\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 23:06:48,364] Trial 34 finished with value: 2.1025444472708354 and parameters: {'batch_size': 8, 'learning_rate': 7.773130376512952e-06, 'weight_decay': 6.918149956423188e-06, 'dropout_rate': 0.1839109526984177, 'lr_decay_factor': 0.8305967978175591, 'lr_patience': 14, 'max_patience': 45, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': True, 'n_components': 114}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0952 | Train Acc: 0.1126 | Val Loss: 2.0909 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 23:08:34,104] Trial 35 finished with value: 2.0909238908349015 and parameters: {'batch_size': 32, 'learning_rate': 1.8255176430650632e-06, 'weight_decay': 0.00024368552223963825, 'dropout_rate': 0.3069107116595323, 'lr_decay_factor': 0.5569793610933377, 'lr_patience': 6, 'max_patience': 57, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': False}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.1584 | Train Acc: 0.1429 | Val Loss: 2.0738 | Val Acc: 0.1707\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 23:17:04,692] Trial 36 finished with value: 2.073775785725291 and parameters: {'batch_size': 4, 'learning_rate': 1.0125264306352175e-06, 'weight_decay': 1.1405410552790383e-07, 'dropout_rate': 0.2549777466069584, 'lr_decay_factor': 0.48739373147356996, 'lr_patience': 24, 'max_patience': 36, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': True, 'n_components': 2}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0776 | Train Acc: 0.1645 | Val Loss: 2.0984 | Val Acc: 0.0976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 23:21:27,738] Trial 37 finished with value: 2.098425946584562 and parameters: {'batch_size': 8, 'learning_rate': 1.1305572010089856e-06, 'weight_decay': 1.4735673693596533e-07, 'dropout_rate': 0.371410949671939, 'lr_decay_factor': 0.4739747481883791, 'lr_patience': 23, 'max_patience': 26, 'model_type': '1DCNN', 'cnn_n_filters_start': 64, 'use_pca': True, 'n_components': 3}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0650 | Train Acc: 0.1385 | Val Loss: 2.0670 | Val Acc: 0.2195\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 23:28:48,798] Trial 38 finished with value: 2.067019538181584 and parameters: {'batch_size': 4, 'learning_rate': 2.439627445686571e-06, 'weight_decay': 1.4401423073502215e-06, 'dropout_rate': 0.24907714420004637, 'lr_decay_factor': 0.37077819413303426, 'lr_patience': 25, 'max_patience': 35, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': True, 'n_components': 82}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0957 | Train Acc: 0.1429 | Val Loss: 2.0898 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 23:37:26,432] Trial 39 finished with value: 2.0897786210222944 and parameters: {'batch_size': 4, 'learning_rate': 1.0065375087817158e-06, 'weight_decay': 1.1618809484005799e-06, 'dropout_rate': 0.25570555248279814, 'lr_decay_factor': 0.3727701776091892, 'lr_patience': 30, 'max_patience': 34, 'model_type': '1DCNN', 'cnn_n_filters_start': 16, 'use_pca': True, 'n_components': 88}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0894 | Train Acc: 0.1385 | Val Loss: 2.0807 | Val Acc: 0.1707\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 23:45:05,664] Trial 40 finished with value: 2.0806728456078507 and parameters: {'batch_size': 4, 'learning_rate': 1.7801288813315735e-06, 'weight_decay': 1.3596795138431986e-06, 'dropout_rate': 0.14596641873011557, 'lr_decay_factor': 0.2629563140953837, 'lr_patience': 25, 'max_patience': 30, 'model_type': 'MLP', 'mlp_n_fc_units': 16, 'use_pca': True, 'n_components': 86}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0709 | Train Acc: 0.1255 | Val Loss: 2.1132 | Val Acc: 0.1463\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 23:47:37,985] Trial 41 finished with value: 2.113229850443398 and parameters: {'batch_size': 16, 'learning_rate': 3.0420576221520736e-06, 'weight_decay': 1.1873532111405244e-07, 'dropout_rate': 0.4322616970878578, 'lr_decay_factor': 0.49458404188733407, 'lr_patience': 30, 'max_patience': 37, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': True, 'n_components': 40}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0472 | Train Acc: 0.1688 | Val Loss: 2.0637 | Val Acc: 0.2439\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-11 23:56:10,024] Trial 42 finished with value: 2.063654597212629 and parameters: {'batch_size': 4, 'learning_rate': 4.252541312931439e-06, 'weight_decay': 2.4315049235248315e-06, 'dropout_rate': 0.33958233099222346, 'lr_decay_factor': 0.39808120888491916, 'lr_patience': 19, 'max_patience': 25, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': True, 'n_components': 85}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0523 | Train Acc: 0.1948 | Val Loss: 2.1035 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:04:15,468] Trial 43 finished with value: 2.103532174738442 and parameters: {'batch_size': 4, 'learning_rate': 2.414901322902848e-06, 'weight_decay': 2.449409915668302e-06, 'dropout_rate': 0.33740512107293513, 'lr_decay_factor': 0.13236011169363482, 'lr_patience': 18, 'max_patience': 25, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': True, 'n_components': 89}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0761 | Train Acc: 0.1602 | Val Loss: 2.0674 | Val Acc: 0.2195\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:12:47,810] Trial 44 finished with value: 2.067440253932302 and parameters: {'batch_size': 4, 'learning_rate': 1.4876126128310077e-06, 'weight_decay': 3.837804014745002e-07, 'dropout_rate': 0.2766230775691823, 'lr_decay_factor': 0.38698755707665133, 'lr_patience': 24, 'max_patience': 24, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': True, 'n_components': 75}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0442 | Train Acc: 0.1688 | Val Loss: 2.1104 | Val Acc: 0.0732\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:23:32,445] Trial 45 finished with value: 2.110424800616939 and parameters: {'batch_size': 4, 'learning_rate': 1.5607620250128261e-06, 'weight_decay': 4.701276223202516e-07, 'dropout_rate': 0.29904016496699776, 'lr_decay_factor': 0.3922864884259078, 'lr_patience': 33, 'max_patience': 20, 'model_type': '1DCNN', 'cnn_n_filters_start': 128, 'use_pca': True, 'n_components': 76}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0896 | Train Acc: 0.1429 | Val Loss: 2.0913 | Val Acc: 0.1463\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:32:56,936] Trial 46 finished with value: 2.0912568859937712 and parameters: {'batch_size': 4, 'learning_rate': 5.350140730046624e-06, 'weight_decay': 2.2995080313532354e-06, 'dropout_rate': 0.37656508795800325, 'lr_decay_factor': 0.2643274101811674, 'lr_patience': 28, 'max_patience': 23, 'model_type': 'MLP', 'mlp_n_fc_units': 64, 'use_pca': True, 'n_components': 101}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/1000 | Train Loss: 2.0306 | Train Acc: 0.2165 | Val Loss: 2.1065 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:33:23,417] Trial 47 finished with value: 2.106507435077574 and parameters: {'batch_size': 16, 'learning_rate': 4.247388203513255e-06, 'weight_decay': 5.288005285151915e-06, 'dropout_rate': 0.2723740100005403, 'lr_decay_factor': 0.404051291972446, 'lr_patience': 19, 'max_patience': 32, 'model_type': 'ROI1DCNN', 'roi_n_filters_start': 64, 'use_pca': True, 'n_components': 55}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.1061 | Train Acc: 0.1255 | Val Loss: 2.0811 | Val Acc: 0.0976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:44:01,179] Trial 48 finished with value: 2.0810722432485442 and parameters: {'batch_size': 4, 'learning_rate': 1.4435529870056305e-06, 'weight_decay': 2.3591174583206712e-07, 'dropout_rate': 0.4068394466759821, 'lr_decay_factor': 0.3565155345170096, 'lr_patience': 38, 'max_patience': 24, 'model_type': 'MLP', 'mlp_n_fc_units': 32, 'use_pca': True, 'n_components': 73}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0986 | Train Acc: 0.1169 | Val Loss: 2.0939 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:46:08,037] Trial 49 finished with value: 2.093854752982535 and parameters: {'batch_size': 32, 'learning_rate': 3.142621127797967e-06, 'weight_decay': 5.667127119559954e-07, 'dropout_rate': 0.21595754559485494, 'lr_decay_factor': 0.27017407792912357, 'lr_patience': 16, 'max_patience': 86, 'model_type': 'MLP', 'mlp_n_fc_units': 16, 'use_pca': True, 'n_components': 101}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/1000 | Train Loss: 2.0674 | Train Acc: 0.1775 | Val Loss: 2.0736 | Val Acc: 0.0976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:46:44,989] Trial 50 finished with value: 2.0736255064243223 and parameters: {'batch_size': 4, 'learning_rate': 9.852913603097554e-06, 'weight_decay': 1.006102052002967e-05, 'dropout_rate': 0.34202556877345636, 'lr_decay_factor': 0.542605821046386, 'lr_patience': 32, 'max_patience': 28, 'model_type': '1DCNN', 'cnn_n_filters_start': 32, 'use_pca': True, 'n_components': 58}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/1000 | Train Loss: 2.0738 | Train Acc: 0.1602 | Val Loss: 2.0720 | Val Acc: 0.0732\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:47:39,697] Trial 51 finished with value: 2.0719504356384277 and parameters: {'batch_size': 4, 'learning_rate': 9.182844339261581e-06, 'weight_decay': 1.6413132297979068e-06, 'dropout_rate': 0.3505818465041091, 'lr_decay_factor': 0.5531184760404567, 'lr_patience': 28, 'max_patience': 29, 'model_type': '1DCNN', 'cnn_n_filters_start': 32, 'use_pca': True, 'n_components': 58}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0563 | Train Acc: 0.1688 | Val Loss: 2.0996 | Val Acc: 0.0732\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 00:57:43,595] Trial 52 finished with value: 2.0996144341259466 and parameters: {'batch_size': 4, 'learning_rate': 4.061897870119359e-06, 'weight_decay': 1.569273498264121e-06, 'dropout_rate': 0.3132511555421764, 'lr_decay_factor': 0.43266436037441136, 'lr_patience': 28, 'max_patience': 32, 'model_type': '1DCNN', 'cnn_n_filters_start': 32, 'use_pca': True, 'n_components': 80}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0691 | Train Acc: 0.1602 | Val Loss: 2.0854 | Val Acc: 0.1707\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 01:04:29,178] Trial 53 finished with value: 2.0854337448027076 and parameters: {'batch_size': 4, 'learning_rate': 2.401060419278497e-06, 'weight_decay': 2.9195879561170533e-07, 'dropout_rate': 0.35822274778198326, 'lr_decay_factor': 0.3006619640898425, 'lr_patience': 22, 'max_patience': 42, 'model_type': '1DCNN', 'cnn_n_filters_start': 32, 'use_pca': True, 'n_components': 64}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/1000 | Train Loss: 2.0453 | Train Acc: 0.2121 | Val Loss: 2.0777 | Val Acc: 0.0976\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 01:05:24,881] Trial 54 finished with value: 2.077720665350193 and parameters: {'batch_size': 4, 'learning_rate': 7.68696204385526e-06, 'weight_decay': 3.324236239379185e-06, 'dropout_rate': 0.3890743309033783, 'lr_decay_factor': 0.5839001965883911, 'lr_patience': 35, 'max_patience': 20, 'model_type': '1DCNN', 'cnn_n_filters_start': 64, 'use_pca': True, 'n_components': 99}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 1.5534 | Train Acc: 0.4935 | Val Loss: 2.4620 | Val Acc: 0.1707\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 01:12:46,558] Trial 55 finished with value: 2.4619641536619605 and parameters: {'batch_size': 4, 'learning_rate': 0.00016153194771342436, 'weight_decay': 7.251232520717534e-07, 'dropout_rate': 0.29217365307270865, 'lr_decay_factor': 0.4711693716629541, 'lr_patience': 25, 'max_patience': 29, 'model_type': 'ROI1DCNN', 'roi_n_filters_start': 32, 'use_pca': True, 'n_components': 48}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0630 | Train Acc: 0.1688 | Val Loss: 2.1048 | Val Acc: 0.1951\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 01:22:00,168] Trial 56 finished with value: 2.1048125813647016 and parameters: {'batch_size': 4, 'learning_rate': 5.764162258279614e-06, 'weight_decay': 2.6116778133764527e-05, 'dropout_rate': 0.43000127411901135, 'lr_decay_factor': 0.5158373698964366, 'lr_patience': 21, 'max_patience': 23, 'model_type': '1DCNN', 'cnn_n_filters_start': 128, 'use_pca': True, 'n_components': 70}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 2.0631 | Train Acc: 0.1732 | Val Loss: 2.1082 | Val Acc: 0.1463\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 01:31:19,933] Trial 57 finished with value: 2.108238604010605 and parameters: {'batch_size': 4, 'learning_rate': 3.488793519671016e-06, 'weight_decay': 9.86414095073367e-07, 'dropout_rate': 0.27052112603510425, 'lr_decay_factor': 0.35431880046029646, 'lr_patience': 27, 'max_patience': 39, 'model_type': 'MLP', 'mlp_n_fc_units': 128, 'use_pca': True, 'n_components': 81}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/1000 | Train Loss: 2.0553 | Train Acc: 0.1775 | Val Loss: 2.0928 | Val Acc: 0.1220\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-12 01:31:31,156] Trial 58 finished with value: 2.092757416934502 and parameters: {'batch_size': 16, 'learning_rate': 1.3885466969781447e-05, 'weight_decay': 2.353632658085107e-06, 'dropout_rate': 0.3229273760610432, 'lr_decay_factor': 0.4238334968637663, 'lr_patience': 12, 'max_patience': 32, 'model_type': 'MLP', 'mlp_n_fc_units': 256, 'use_pca': True, 'n_components': 92}. Best is trial 22 with value: 2.054228852434856.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 755/1000 | Train Loss: 2.0451 | Train Acc: 0.1861 | Val Loss: 2.1141 | Val Acc: 0.1951\r"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92349b25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
